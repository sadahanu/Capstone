{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from getvector import getvector\n",
    "from tensorflow.python.platform import gfile\n",
    "import time\n",
    "#from progress.bar import Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMGDIR = \"/Users/zhouyu/Documents/Zhou_Yu/DS/Galvanize/Capstone_data/test_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build data inputs and labels\n",
    "data_inputs = []\n",
    "data_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " on average takes 13.3693586086s to conver one image\n"
     ]
    }
   ],
   "source": [
    "# JPG images --> Inception-V3 --> 2048-dimensional vector, sequential method\n",
    "image_dir = IMGDIR\n",
    "file_list = []\n",
    "file_glob = os.path.join(image_dir, '*.jpg')\n",
    "file_list.extend(gfile.Glob(file_glob))\n",
    "\n",
    "file_list = file_list[0:300]\n",
    "start_time = time.time()\n",
    "for file_name in file_list:\n",
    "    data_inputs.append(getvector(file_name))\n",
    "    if 'basset' in file_name:\n",
    "        data_labels.append([1, 0])\n",
    "    else:\n",
    "        data_labels.append([0, 1])\n",
    "print \" on average takes {}s to conver one image\".format((time.time() - start_time)/len(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('data_inputs.txt', data_inputs)\n",
    "np.savetxt('data_labels.txt', data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if figures data already exists:\n",
    "if os.path.isfile('./data_inputs.txt') and os.path.isfile('./data_labels.txt'):\n",
    "    data_inputs = np.loadtxt('data_inputs.txt')\n",
    "    data_labels = np.loadtxt('data_labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build models\n",
    "# Splitting into train, val, and test\n",
    "train_inputs, valtest_inputs, train_labels, valtest_labels = train_test_split(data_inputs, data_labels, test_size=0.3, \n",
    "                                                                              random_state=42, stratify=data_labels)\n",
    "val_inputs, test_inputs, val_labels, test_labels = train_test_split(valtest_inputs, valtest_labels, test_size=0.4, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting hyperparameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 16\n",
    "epochs = 10\n",
    "log_batch_step = 50\n",
    "\n",
    "# useful info\n",
    "n_features = np.size(train_inputs, 1)\n",
    "n_labels = np.size(train_labels, 1)\n",
    "\n",
    "# Placeholders for input features and labels\n",
    "inputs = tf.placeholder(tf.float32, (None, n_features))\n",
    "labels = tf.placeholder(tf.float32, (None, n_labels))\n",
    "\n",
    "# Setting up weights and bias\n",
    "weights = tf.Variable(tf.truncated_normal((n_features, n_labels), stddev=0.1), name='weights')\n",
    "bias = tf.Variable(tf.zeros(n_labels), name='bias')\n",
    "tf.add_to_collection('vars', weights)\n",
    "tf.add_to_collection('vars', bias)\n",
    "\n",
    "# Setting up operation in fully connected layer\n",
    "logits = tf.add(tf.matmul(inputs, weights), bias)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "tf.add_to_collection('pred', prediction)\n",
    "\n",
    "# Defining loss of network\n",
    "difference = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "loss = tf.reduce_sum(difference)\n",
    "\n",
    "# Setting optimiser\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# Define accuracy\n",
    "is_correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct_prediction, tf.float32))\n",
    "\n",
    "saver = tf.train.Saver((weights, bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver((weights, bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1/10: 100%|██████████| 1/1 [00:00<00:00,  2.47batches/s]\n",
      "Epoch  2/10: 100%|██████████| 1/1 [00:00<00:00, 611.24batches/s]\n",
      "Epoch  3/10: 100%|██████████| 1/1 [00:00<00:00, 484.27batches/s]\n",
      "Epoch  4/10: 100%|██████████| 1/1 [00:00<00:00, 559.61batches/s]\n",
      "Epoch  5/10: 100%|██████████| 1/1 [00:00<00:00, 467.54batches/s]\n",
      "Epoch  6/10: 100%|██████████| 1/1 [00:00<00:00, 488.79batches/s]\n",
      "Epoch  7/10: 100%|██████████| 1/1 [00:00<00:00, 521.42batches/s]\n",
      "Epoch  8/10: 100%|██████████| 1/1 [00:00<00:00, 366.57batches/s]\n",
      "Epoch  9/10: 100%|██████████| 1/1 [00:00<00:00, 573.78batches/s]\n",
      "Epoch 10/10: 100%|██████████| 1/1 [00:00<00:00, 519.23batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 1, Loss: 1.18230748177, Accuracy: 0.857142865658\n",
      "After epoch 2, Loss: 0.0635327994823, Accuracy: 1.0\n",
      "After epoch 3, Loss: 0.0126980803907, Accuracy: 1.0\n",
      "After epoch 4, Loss: 0.00716634700075, Accuracy: 1.0\n",
      "After epoch 5, Loss: 0.00531095638871, Accuracy: 1.0\n",
      "After epoch 6, Loss: 0.00423100730404, Accuracy: 1.0\n",
      "After epoch 7, Loss: 0.00348590756766, Accuracy: 1.0\n",
      "After epoch 8, Loss: 0.00293840956874, Accuracy: 1.0\n",
      "After epoch 9, Loss: 0.00252217100933, Accuracy: 1.0\n",
      "After epoch 10, Loss: 0.00219784188084, Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# training \n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Running the training in batches\n",
    "    batch_count = int(math.ceil(len(train_inputs)/batch_size))\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        batches_pbar = tqdm(range(batch_count), desc='Epoch {:>2}/{}'.format(epoch_i+1, epochs), unit='batches')\n",
    "        # The training cycle\n",
    "        for batch_i in batches_pbar:\n",
    "            # Get a batch of training features and labels\n",
    "            batch_start = batch_i*batch_size\n",
    "            batch_inputs = train_inputs[batch_start:batch_start + batch_size]\n",
    "            batch_labels = train_labels[batch_start:batch_start + batch_size]\n",
    "            # Run optimizer\n",
    "            _ = sess.run(optimizer, feed_dict={inputs: batch_inputs, labels: batch_labels})\n",
    "\n",
    "        # Check accuracy against validation data\n",
    "        val_accuracy, val_loss = sess.run([accuracy, loss], feed_dict={inputs: val_inputs, labels: val_labels})\n",
    "        print(\"After epoch {}, Loss: {}, Accuracy: {}\".format(epoch_i+1, val_loss, val_accuracy))\n",
    "\n",
    "    g = tf.get_default_graph()\n",
    "    saver.save(sess, 'testsave')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zhouyu/Documents/Zhou_Yu/DS/Galvanize/Capstone_data/test_of_test/american_pit_bull_terrier_112.jpg\n"
     ]
    }
   ],
   "source": [
    "# to make one test prediction\n",
    "TESTDIR = \"/Users/zhouyu/Documents/Zhou_Yu/DS/Galvanize/Capstone_data/test_of_test\"\n",
    "test_list = []\n",
    "test_glob = os.path.join(TESTDIR, '*.jpg')\n",
    "test_list.extend(gfile.Glob(test_glob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zhouyu/Documents/Zhou_Yu/DS/Galvanize/Capstone_data/test_of_test/american_pit_bull_terrier_111.jpg\n"
     ]
    }
   ],
   "source": [
    "test_file_name = test_list[0]\n",
    "print test_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_input = getvector(file_name).reshape((1,2048))\n",
    "with tf.Session() as sess:\n",
    "    new_saver = tf.train.import_meta_graph('testsave.meta')\n",
    "    new_saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "\n",
    "    prediction = sess.run(prediction, feed_dict={inputs: image_input})\n",
    "    predict_res = prediction.eval()\n",
    "    print ('It\\'s a Basset: {}, It\\'s a Pit Bull: {}'.format(predict_res[0][0], predict_res[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
